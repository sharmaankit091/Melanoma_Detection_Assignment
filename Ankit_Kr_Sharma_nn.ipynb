{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6cedc19-5c5a-45aa-8ef0-820d7796c324",
   "metadata": {},
   "source": [
    "## Problem statement:\n",
    "\n",
    "To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b642f-5f24-4183-87b1-bdd4aa13a9d6",
   "metadata": {},
   "source": [
    "### Importing Skin Cancer Data\n",
    "#### To do: Take necessary actions to read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d4201-e43d-4735-9344-b29d18afa31e",
   "metadata": {},
   "source": [
    "### Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1329b-4ec2-4d4a-8160-60b3f9292ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for read/write operations\n",
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "# Libraries for calculations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for graphical visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# libraries for machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d04d3-34a9-4247-8c82-7db404a6df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using the data by mounting the google drive, uncomment and use the use the following :\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "##Ref:https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfa60f-7774-47e8-ac36-a631de510a40",
   "metadata": {},
   "source": [
    "This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07879f05-e61c-4ffc-8d4e-577e7fcd5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've used my local windows based system for the assignment. Kindly update path in the variable 'local_dir_path' as required during evaluation.\n",
    "# Please don't add Train and Test in the below path as it will be populated by itself as per logic in the following cell.\n",
    "\n",
    "dataset_dir_path = r\"datasets\"\n",
    "local_dir_path = os.path.join(dataset_dir_path, \"Skin cancer ISIC The International Skin Imaging Collaboration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f476e-9993-4bfd-bb2c-3a52029a39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path for train and test images\n",
    "\n",
    "data_dir_train = pathlib.Path(f\"{local_dir_path}/Train\")\n",
    "data_dir_test = pathlib.Path(f\"{local_dir_path}/Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8d584-ef2b-446f-8a56-438db787af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the count of images within the Train and Test Files respectively\n",
    "\n",
    "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
    "print(image_count_train)\n",
    "\n",
    "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
    "print(image_count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4297b7-2a4a-40f6-86fe-f7575a178b9d",
   "metadata": {},
   "source": [
    "### Load using keras.preprocessing\n",
    "\n",
    "Let's load these images off disk using the helpful image_dataset_from_directory utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb685145-df3c-4296-8a86-30eaabe10c85",
   "metadata": {},
   "source": [
    "### Create a dataset\n",
    "\n",
    "Define some parameters for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf551410-c3b7-4097-a3c8-4740d373080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccb657-179b-43a1-8a8d-d2d895c4a2bf",
   "metadata": {},
   "source": [
    "Use 80% of the images for training, and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13635f7f-cd1b-48e5-be64-4d19e2326901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset logic\n",
    "# Initialized 'seed = 123' while creating the dataset using tf.keras.preprocessing.image_dataset_from_directory as per instructions in the Starter Notebook\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir_train,\n",
    "    seed = 123,\n",
    "    validation_split = 0.2,  # 80% sample for training and 20% for validation\n",
    "    subset = 'training',\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7360d-c51f-4a51-ab33-193d9c19e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Dataset logic\n",
    "# Initialized 'seed = 123' while creating the dataset using tf.keras.preprocessing.image_dataset_from_directory as per instructions in the Starter Notebook\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir_train,\n",
    "    seed = 123,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'validation',\n",
    "    image_size =(img_height,img_width),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62ac92-5889-4fbb-9a82-512194a92e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing all the classes Names. These correspond to the directory names in alphabetical order.\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1ff25-b2b6-4673-be0b-e5ac078101ff",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2e495-79d7-4f9b-9462-6f4458e1a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475aa33f-e0b4-4f07-b6de-cd2d49c71644",
   "metadata": {},
   "source": [
    "An **image batch** is a collection of images stacked together along the batch dimension. In **machine learning and deep learning**, models often process data in batches rather than individual samples.\n",
    "\n",
    "For example, in a batch of images, each image might have dimensions **(height, width, channels)**, and if the batch size is **32**, the shape of the batch tensor would be **(32, height, width, channels)**. This allows the model to process multiple images simultaneously, which can improve efficiency and speed during training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97662742-d51c-4840-9f18-c08dc82bb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffbd83e-c5c8-40b9-89a9-17ba8bbbf0da",
   "metadata": {},
   "source": [
    "From the above cell output, it's clear that it is an image batch of **32 images** of shape **180 x 180 x 3**. \r\n",
    "The last dimension refers to color channels **RGB (Red, Blue, Green)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbebad76-e13f-438c-98c4-465cf5f6b134",
   "metadata": {},
   "source": [
    "**AUTOTUNE** is a special value that allows TensorFlow to automatically tune the prefetch buffer size dynamically at runtime based on the available memory and other factors. \r\n",
    "This can help optimize the performance of your input pipeline without manually tuning the buffer size.\r\n",
    "\r\n",
    "**Dataset.cache** keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\r\n",
    "\r\n",
    "**Dataset.prefetch** overlaps data preprocessing and model execution while trainingg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d52965-b39e-4179-a988-70108b8f991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb506fe8-9e94-4a25-8039-f9a122b93853",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "Create a CNN model, which can accurately detect 9 classes present in the dataset. \n",
    "Use ```layers.experimental.preprocessing.Rescaling``` to normalize pixel values between (0,1). \n",
    "The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network.\n",
    "Here, it is good to standardize values to be in the `[0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee16582-1f05-4241-97fa-1d07d237204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total classes are 9\n",
    "num_classes = 9\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Rescaling(1. / 255, input_shape = (img_height, img_width, 3)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        \n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3db16-6fc4-4667-8e6a-70314c169fe4",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "Choose an appropirate optimiser and loss function for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fdc65-dd78-4dc8-9652-ef3dbef8af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159e1b0-473a-4276-a85b-33facbf2cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the summary of all layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad08d1-4dff-4d01-8f9e-00f637fd11ce",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7dd03-6022-483a-a680-02a34fd6ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7976198-0e26-47f9-8ca3-0cbf0dc47878",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ec5d1-0b75-45b6-a5ea-4cb26ebb0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b0369-90a2-43fb-b89b-94163ba74aac",
   "metadata": {},
   "source": [
    "#### Write your findings after the model fit, see if there is an evidence of model overfit or underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c52bbe-d05b-4da0-b898-af134f487712",
   "metadata": {},
   "source": [
    "**Observations and Findings**\n",
    "\n",
    "1. The model's training accuracy rose steadily at first but declined slightly at **$14^{th}$ epoch** and then steadily rose again upto **88%**.\n",
    "2. The model's validation accuracy was fluctuating as it first rose then declined slightly and then increased upto **54%** .\n",
    "3. The model's training loss steadily declined.\n",
    "4. The model's validation loss shows a U curve shape where it declined first and then increased.\n",
    "5. The model's high training accuracy and low validation accuracy indicate overfitting as it managed to capture noise and details in the data.\n",
    "\n",
    "As a result we'll need to modify the existing training data using data Augmentation techniques which involves adjusting the data slightly by rotation, flipping, zooming in/out etc. and then train the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e0b56-1f0e-4fd9-9fa1-f15080be90e7",
   "metadata": {},
   "source": [
    "### Augment the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a734110-91f2-4fbc-90b5-a3ab497db8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", input_shape = (img_height, img_width, 3)),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.2, fill_mode = 'reflect'),\n",
    "        layers.experimental.preprocessing.RandomZoom(0.2, fill_mode = 'reflect')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec9af1-03d4-40b0-b7ea-0eee6397fd7e",
   "metadata": {},
   "source": [
    "### Visualize the Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba62e28-f878-41bb-90b6-ff83593864b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how your augmentation strategy works for one instance of training image.\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_data(images)[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659a14c-09d2-4a0c-9d9f-1161c87d1638",
   "metadata": {},
   "source": [
    "### Create the model using Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895b822-52c8-431c-b8ff-b59da98af3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        augmented_data,\n",
    "        tf.keras.layers.Rescaling(1. / 255, input_shape = (img_height, img_width, 3)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        \n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccaf46-311f-4bc4-ab71-799e83851ab2",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e10014-9f37-4895-96b1-e805b1215607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55053be-225b-4cfc-be49-6b641ee7031c",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb399bd9-c905-48d5-800c-a2e48a7dcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs = epochs    # Declared already in earlier coding steps so using same value here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad47df7-0099-4ec4-9366-0ee338ff5c99",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f4d96-681a-4585-b481-6c3c78a630dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a2693-f486-4737-93d3-a806e0534da8",
   "metadata": {},
   "source": [
    "#### Write your findings after the model fit, see if there is an evidence of model overfit or underfit. Do you think there is some improvement now as compared to the previous model run?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f6554-7f5b-4370-960a-9471a97e8091",
   "metadata": {},
   "source": [
    "**Observations and Findings**\n",
    "\n",
    "1. The model's training and validation accuracy post data augmentation are now along similar lines.\n",
    "2. The model's training loss steadily declined.\n",
    "4. The model's validation loss is more compared to training loss.\n",
    "5. The model's training and validation accuracies are both low thus model is underfitting.\n",
    "\n",
    "As a result we'll now try another approach to check if these results can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765ca98-f0af-4f15-8756-eda92ede3d2b",
   "metadata": {},
   "source": [
    "#### Find the distribution of classes in the training dataset.\n",
    "#### **Context:** Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef85cf-c391-48d3-8105-b6273f2b7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = []\n",
    "class_lables_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a0b2e-d0ca-4511-8d5a-166d0f4e72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_images = os.path.abspath(os.path.join(data_dir_train, '**/*.jpg'))\n",
    "list_of_images = glob.glob(required_images, recursive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bb7ed-6f9c-4980-8c0c-a4eb4ee3d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in list_of_images:\n",
    "    class_label = os.path.basename(os.path.dirname(image))\n",
    "    class_lables_list.append(class_label)\n",
    "    path_list.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585e94c-0dd2-469b-ad16-3cff8025c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels_path_df = pd.DataFrame(\n",
    "    dict(\n",
    "        class_label = class_lables_list,\n",
    "        image_path = path_list\n",
    "    )\n",
    ")\n",
    "class_labels_path_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921dab5-1e7b-4e6c-9590-40ad20509e12",
   "metadata": {},
   "source": [
    "### Visualize the Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a3bfc-67c3-4cdc-8834-0a92b61d13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart to visualize percenatge wise class distribution\n",
    "\n",
    "plt.figure(figsize = (7, 7))\n",
    "class_distribution_value_counts = class_labels_path_df.class_label.value_counts(ascending = True)\n",
    "plt.pie(class_distribution_value_counts.values, labels = class_distribution_value_counts.index, autopct='%.2f%%', startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.title('Class Distribution', pad=30, loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec9514-d8a1-431c-be5b-54fbf106be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn Count Plot to visualize class distribution\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.countplot(y = \"class_label\", data = class_labels_path_df, order=class_distribution_value_counts.index, palette = \"Set1\", hue = \"class_label\")\n",
    "plt.xlabel('Count of Images in a particular Class type', fontsize = 10)\n",
    "plt.ylabel('Class types', fontsize = 10)\n",
    "plt.title('Count of Images belonging to a class Type v/s Type of class', fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ed2bc-5a3d-47ca-a394-89a657f3074a",
   "metadata": {},
   "source": [
    "#### Write your findings here: \n",
    "#### - Which class has the least number of samples?\n",
    "#### - Which classes dominate the data in terms proportionate number of samples?\n",
    "\n",
    "**Observations and Findings:**\n",
    "- Based upon the above visualizations we can see that there's a clear case of class imbalance.\n",
    "- **seborrheic keratosis** class has the least number of samples (**3.44%**).\n",
    "- **pigmented benign keratosis** with **20.63%** dominates the classes followed by **melanoma** with **19.56%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f072a85-e6f3-45d1-bd08-7c28b96606bd",
   "metadata": {},
   "source": [
    "#### Rectify the class imbalance\n",
    "#### **Context:** You can use a python package known as `Augmentor` (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055cca3-c920-418b-9338-896c3c4e8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e9381-c5b2-4175-9760-65a058725f9e",
   "metadata": {},
   "source": [
    "To use `Augmentor`, the following general procedure is followed:\n",
    "\n",
    "1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n",
    "2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n",
    "3. Execute these operations by calling the `Pipelineâ€™s` `sample()` method.\n",
    "hod.hod.hod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af7c81-2f7f-4c75-a8ef-4f4de49bef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training_dataset = str(data_dir_train) + \"/\"\n",
    "\n",
    "import Augmentor\n",
    "\n",
    "for i in class_names:\n",
    "    p = Augmentor.Pipeline(path_to_training_dataset + i,save_format='jpg')\n",
    "    p.rotate(probability = 0.7, max_left_rotation = 10, max_right_rotation = 10)\n",
    "    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffc11c-08f1-4f6b-b9d1-bb2d0c28dbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84dabd1-3314-4ff3-8b0e-c00817dd5394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b29146-184f-4d4f-a9c3-c7c9fc4eeed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba45835-a6d1-4b3a-8a88-d4d5e078bfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc4671-c721-49ef-83bf-8f2329801c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec931dfa-a225-4379-88b9-284e6ecddef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85f279-b120-471f-a038-78a97cdf499b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
